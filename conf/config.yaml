# defaults
defaults:
  - model: band_split_rope_transformer
  - loss: default
  - train_dataset: default
  - val_dataset: default
  - test_dataset: default
  - sad: default
  - augmentations: default
  - callbacks: default
  - _self_

# data
train_multiple_dataset:
  musdbDataset: True
  moisesdbDataset: True
  slakh2100Dataset: True
val_multiple_dataset:
  musdbDataset: True
  moisesdbDataset: True
  slakh2100Dataset: True
subset:
  # 86502 * 0.01 = 865
  # 865 / 8 = 108
  ratio: 0.10
check_nan:
  musdbDataset: False
  moisesdbDataset: False
  slakh2100Dataset: False

train_loader:
  batch_size: 1
  num_workers: 16
  shuffle: True
  drop_last: True
val_loader:
  batch_size: 1
  num_workers: 16
  shuffle: False
  drop_last: True

# optimization
opt:
  _target_: torch.optim.AdamW
  lr: 5e-4

# scheduler
sch:
    warmup_step: 4000
    alpha: 0.999
    step_size: 8000
    gamma: 0.99

ckpt_path: null

logger:
  _target_: pytorch_lightning.loggers.TensorBoardLogger
  save_dir: "/tb_logs"
  name: ""
  version: ""
  default_hp_metric: False

trainer:
  fast_dev_run: False
  max_epochs: 512
  check_val_every_n_epoch: 1
  num_sanity_val_steps: 16
  log_every_n_steps: 64
  accelerator: "auto"
  devices: 8
  gradient_clip_val: 5
  precision: 16
  enable_progress_bar: True
  benchmark: True
  deterministic: False

# hydra
experiment_dirname: band-split-rope-transformer
hydra:
  run:
    dir: logs/${...experiment_dirname}/${now:%Y-%m-%d}_${now:%H-%M}
  job:
    chdir: False
    